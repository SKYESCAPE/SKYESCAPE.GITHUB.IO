---
layout: post
title: 算法整理——递推最小二乘(RLS)
date: 2020-03-08
categories: blog
tags: [RLS,算法,最小二乘,系统辨识]
description: 为了让自己这个记性差的人可以方便的查找语法，自己编辑一个存档。
---
[TOC]
# 一、最小二乘到递推最小二乘
对于以下数据,若已经多组$x_i$和y数据，如何辨识参数$\Theta$(假设参数定常)   
$$y=\begin{bmatrix}x_1 & \cdots & x_n \end{bmatrix} \begin{bmatrix} \theta_1 \\ \vdots \\ \theta_n \end{bmatrix} = \Phi \cdot \Theta $$  

将以上公式写为k组数据紧致的形式：  
$$Y_k= \begin{bmatrix} y_1 \\ \vdots \\ y_k \end{bmatrix} =  \begin{bmatrix} \phi_1 \\ \vdots \\ \phi_{k} \end{bmatrix} \Theta = \begin{bmatrix} x_1^1 & \cdots & x_n^1 \\ \vdots & \cdots & \vdots \\ x_1^{k} & \cdots & x_n^{k} \end{bmatrix} \Theta = \Phi_k \cdot \Theta $$  

其最小二乘解为：  
$$\hat{\theta}_k=(\Phi_k^T \Phi_k)^{-1} \Phi_k^T Y_k$$  

该最小二乘解运算量大，若数据连续不断，每次都计算该最小二乘解是相当麻烦的，若能转化为递推形式，即新的参数=上一次参数+修正项，则可减少许多计算量，实现实时辨识。设在n时刻和n-1时刻有下列估计：  
$$\hat{\theta}_n=(\Phi_n^T \Phi_n)^{-1} \Phi_n^T Y_n$$   

$$\hat{\theta}_{n-1}=(\Phi_{n-1}^T \Phi_{n-1})^{-1} \Phi_{n-1}^T Y_{n-1}$$   

假设在第n-1次递推中，已计算得到$\hat{\theta}_{n-1}$,则在第n次递推时，已知观测向量$\phi(n-1)$和y(n)，记  
$$\Phi(n-1)=[\phi(1),\cdots,\phi(n-1)]^T$$  

$$\Phi(n)=[\phi(1),\cdots,\phi(n)]^T=[\Phi^T(n-1),\phi(n)]^T$$  

$$Y(n-1)=[y(1),\cdots,y(n-1)]^T$$  

$$Y(n)=[y(1),\cdots,y(n)]^T=[Y^T(n-1),y(n)]^T$$  

其中，根据最小二乘解，设协方差矩阵
$$\begin{aligned} 
P(n)&=(\Phi^T(n) \Phi(n))^{-1} \\
      &=([\Phi^T(n-1),\phi(n)] [\Phi^T(n-1),\phi(n)]^T ) ^{-1} \\
      &=([\Phi^T(n-1) \Phi^T(n-1)+\phi(n) \phi^T(n) ) ^{-1} \\
      &=([P^{-1}(n-1)+\phi(n) \phi^T(n) ) ^{-1}
\end{aligned} $$  

根据反演公式$(A+BCD)^{-1}=A^{-1}-A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}$可得：

$$\begin{aligned} 
P(n)&=([P^{-1}(n-1)+\phi(n) \phi^T(n) ) ^{-1} \\
      &=P(n-1)-P(n-1)\phi(n) [1+ \phi^T(n) P(n-1) \phi(n)]^{-1} \phi^T(n) P(n-1)\\
      &=[I- \frac{P(n-1)\phi(n)\phi^T(n)}{1+ \phi^T(n) P(n-1) \phi(n)}]P(n-1)
\end{aligned} $$ 

其中，$1+ \phi^T(n) P(n-1) \phi(n)$为一标量。根据最小二乘解可按下列推导：

$$\begin{aligned} 
\hat{\theta}(n)&=(\Phi^T(n) \Phi(n))^{-1} \Phi^T(n) Y(n) \\
               &=P(n) \Phi^T(n) Y(n) \\
               &=P(n) [\Phi^T(n-1), \phi(n)] [Y^T(n-1), y(n)]^T \\
               &=P(n) [\Phi^T(n-1) Y^T(n-1) + \phi(n) y(n)] \\
               &=P(n) [P^{-1}(n-1) \hat{\theta}(n-1) + \phi(n) y(n)] \\
               &=P(n) \{[P^{-1}(n)- \phi(n) \phi^T(n)]\hat{\theta}(n-1) + \phi(n) y(n) \} \\
               &= \hat{\theta}(n-1) + P(n) \phi(n) [y(n) - \phi^T(n) \hat{\theta}(n-1)]
\end{aligned} $$ 

至此，已得到最小二乘的递推形式，$y(n) - \phi^T(n) \hat{\theta}(n-1)$可视为与上一次估计的偏差，$P(n) \phi(n)$可视为修正系数。整理可得一下递推式：

$$\hat{\theta}(n)=\hat{\theta}(n-1) + K(n) [y(n) - \phi^T(n) \hat{\theta}(n-1)]$$ 

$$P(n)=[I- K(n)\phi^T(n)]P(n-1)$$   

$$K(n)=\frac{P(n-1) \phi(n)}{1+ \phi^T(n) P(n-1) \phi(n)} $$ 

其中，$K(n)$为增益矩阵，计算顺序为K→P→$\theta$，为保证P(n)为对称非增矩阵，P(n)可采用下式计算

$$P(n)=P(n-1)-K(n)K^T(n)[1+ \phi^T(n) P(n-1) \phi(n)]$$   

# 二、递推最小二乘的一些注意事项
## 1.递推初始值
递推初值是一个比较重要的问题，一般有两种方法：  
①取$\theta(0)$为零或较小的参数，取 $ P(0)=- \alpha^2 I $,其中$\alpha$为较大实数。
②将大于参数个数的L组数据用最小二乘法求初值再迭代。
## 2.数据饱和问题
P(n)随着递推衰减使得K(n)也衰减，失去修正能力称为数据饱和。主要修正方法有：  
①遗忘因子法：见第三部分  
②协方差重调：指定时刻重新调整P(n)
③协方差修正：设定阈值修正P(n)  
④限定记忆法：增加新数据去除旧数据  
## 3.计算复杂性
进行一次递推共需要加减法$n^3+3n$次，乘除法$n^3+2n^2+4n$次

# 三、加权RLS和遗忘因子RLS
## 1.加权RLS
加权矩阵$\Lambda_n = diag \{ \lambda_1,\cdots,\lambda_n \}$,令$P(n)=(\Phi^T(n) \Lambda_n \Phi(n))^{-1}$，推导过程同上，此时，

$$K(n)=\frac{P(n-1) \phi(n)}{\lambda_n^{-1}+ \phi^T(n) P(n-1) \phi(n)} $$   

其余项与普通RLS相同。

## 2.遗忘因子RLS
将加权RLS的加权矩阵替换为$\Lambda_n = diag \{ \lambda^n,\lambda^{n-1},\cdots,\lambda,1 \}$,此时，各时刻的权重不同，与时间相关，能够跟踪缓慢的时变特性，相同的推导可得到下式：  

$$K(n)=\frac{P(n-1) \phi(n)}{\lambda+ \phi^T(n) P(n-1) \phi(n)} $$  

$$P(n)=\frac{1}{\lambda}[I- K(n)\phi^T(n)]P(n-1)$$  

其余项与普通RLS相同。对于遗忘因子$0<\lambda<1$有以下讨论：  
①$\lambda=1$时，与普通RLS相同。  
②$\lambda$较小时，由于加权为指数次，则遗忘速度快，对噪声敏感。  
③$\lambda$典型值为(0.95,0.99)

## 3.变遗忘因子RLS
欲对快时变参数进行估计，考虑对遗忘因子进行动态调整，当误差小于均值是，调大遗忘因子，反之则减小。可设变遗忘因子F(n)如下：  

$$F(n)=1-[1- \frac{\phi^T(n-l_0) P(n) \phi(n-l_0)}{1+\phi^T(n-l_0) P(n) \phi(n-l_0)}] \frac{e^2(t)}{R}$$

其中，e(t)为上述偏差，R为固定常数，则相应的RLS为：

$$K(n)=\frac{P(n-1) \phi(n)}{F(n)+ \phi^T(n) P(n-1) \phi(n)} $$  

$$P(n)=\frac{1}{F(n)}[I- K(n)\phi^T(n)]P(n-1)$$  

其余项与普通RLS相同。